{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1986ca64",
   "metadata": {},
   "source": [
    "# Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import evaluate\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c4f091",
   "metadata": {},
   "source": [
    "# Relative Paths for models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"../models/\"\n",
    "data_path = \"../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d3096",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de63d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45abf6c398c8412d8d46d2c7c6b3aec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    data_path + \"FilteredReviews.csv\", usecols=[\"Id\", \"Summary\", \"Text\", \"ProductId\"]\n",
    ")\n",
    "df.dropna(subset=[\"Summary\", \"Text\"], inplace=True)\n",
    "\n",
    "df = df.sample(10000, random_state=42)\n",
    "\n",
    "df = df.rename(columns={\"Summary\": \"target_text\", \"Text\": \"input_text\"})\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Step 4: Preprocessing function (from this paper for encoding summarization task page 47: https://arxiv.org/pdf/1910.10683)\n",
    "\n",
    "\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"input_text\"]\n",
    "    model_inputs = tokenizer(\n",
    "        input_text, max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        example[\"target_text\"], max_length=64, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "# Train/test Split\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74abeaf7",
   "metadata": {},
   "source": [
    "# Finetuning T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5846e88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cea3c512ced448f9fe39c1b9e920ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.57, 'grad_norm': 1.0318259000778198, 'learning_rate': 4.8562962962962964e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6033, 'grad_norm': 0.8597363829612732, 'learning_rate': 4.708148148148148e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4909, 'grad_norm': 0.7873992919921875, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4401, 'grad_norm': 0.6481276750564575, 'learning_rate': 4.411851851851852e-05, 'epoch': 0.36}\n",
      "{'loss': 0.4497, 'grad_norm': 0.5863837003707886, 'learning_rate': 4.263703703703704e-05, 'epoch': 0.44}\n",
      "{'loss': 0.4617, 'grad_norm': 0.5111042261123657, 'learning_rate': 4.115555555555556e-05, 'epoch': 0.53}\n",
      "{'loss': 0.4511, 'grad_norm': 0.6567074656486511, 'learning_rate': 3.967407407407408e-05, 'epoch': 0.62}\n",
      "{'loss': 0.4448, 'grad_norm': 0.7687165141105652, 'learning_rate': 3.8192592592592594e-05, 'epoch': 0.71}\n",
      "{'loss': 0.45, 'grad_norm': 0.45338940620422363, 'learning_rate': 3.671111111111111e-05, 'epoch': 0.8}\n",
      "{'loss': 0.4129, 'grad_norm': 0.4568389654159546, 'learning_rate': 3.522962962962963e-05, 'epoch': 0.89}\n",
      "{'loss': 0.449, 'grad_norm': 0.6473798155784607, 'learning_rate': 3.374814814814815e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9a27d4c16c4a2c918a78016319c42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.40407595038414, 'eval_runtime': 11.4445, 'eval_samples_per_second': 87.378, 'eval_steps_per_second': 10.922, 'epoch': 1.0}\n",
      "{'loss': 0.4319, 'grad_norm': 0.5494844317436218, 'learning_rate': 3.226666666666667e-05, 'epoch': 1.07}\n",
      "{'loss': 0.4128, 'grad_norm': 0.5478624701499939, 'learning_rate': 3.078518518518519e-05, 'epoch': 1.16}\n",
      "{'loss': 0.426, 'grad_norm': 0.4588949382305145, 'learning_rate': 2.9303703703703704e-05, 'epoch': 1.24}\n",
      "{'loss': 0.4261, 'grad_norm': 0.5116302371025085, 'learning_rate': 2.782222222222222e-05, 'epoch': 1.33}\n",
      "{'loss': 0.4145, 'grad_norm': 0.43846842646598816, 'learning_rate': 2.6340740740740744e-05, 'epoch': 1.42}\n",
      "{'loss': 0.4048, 'grad_norm': 1.2250356674194336, 'learning_rate': 2.485925925925926e-05, 'epoch': 1.51}\n",
      "{'loss': 0.4215, 'grad_norm': 0.6696498990058899, 'learning_rate': 2.337777777777778e-05, 'epoch': 1.6}\n",
      "{'loss': 0.4101, 'grad_norm': 0.5202645063400269, 'learning_rate': 2.1896296296296297e-05, 'epoch': 1.69}\n",
      "{'loss': 0.4105, 'grad_norm': 0.48745688796043396, 'learning_rate': 2.0414814814814817e-05, 'epoch': 1.78}\n",
      "{'loss': 0.4247, 'grad_norm': 0.8214144110679626, 'learning_rate': 1.8933333333333334e-05, 'epoch': 1.87}\n",
      "{'loss': 0.4249, 'grad_norm': 0.7195630073547363, 'learning_rate': 1.7451851851851854e-05, 'epoch': 1.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39b1beda0dd4b1c96c5eb905fd0f7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3983560800552368, 'eval_runtime': 16.7363, 'eval_samples_per_second': 59.75, 'eval_steps_per_second': 7.469, 'epoch': 2.0}\n",
      "{'loss': 0.4049, 'grad_norm': 0.5594578981399536, 'learning_rate': 1.597037037037037e-05, 'epoch': 2.04}\n",
      "{'loss': 0.4007, 'grad_norm': 0.5810642242431641, 'learning_rate': 1.448888888888889e-05, 'epoch': 2.13}\n",
      "{'loss': 0.4294, 'grad_norm': 0.5252114534378052, 'learning_rate': 1.3007407407407407e-05, 'epoch': 2.22}\n",
      "{'loss': 0.4099, 'grad_norm': 0.5973052978515625, 'learning_rate': 1.1525925925925926e-05, 'epoch': 2.31}\n",
      "{'loss': 0.405, 'grad_norm': 0.5804758071899414, 'learning_rate': 1.0044444444444446e-05, 'epoch': 2.4}\n",
      "{'loss': 0.4284, 'grad_norm': 0.5606054663658142, 'learning_rate': 8.562962962962962e-06, 'epoch': 2.49}\n",
      "{'loss': 0.4244, 'grad_norm': 0.7815588712692261, 'learning_rate': 7.081481481481482e-06, 'epoch': 2.58}\n",
      "{'loss': 0.3999, 'grad_norm': 0.40906208753585815, 'learning_rate': 5.600000000000001e-06, 'epoch': 2.67}\n",
      "{'loss': 0.4176, 'grad_norm': 0.5159937143325806, 'learning_rate': 4.118518518518519e-06, 'epoch': 2.76}\n",
      "{'loss': 0.4004, 'grad_norm': 0.4270871877670288, 'learning_rate': 2.6370370370370373e-06, 'epoch': 2.84}\n",
      "{'loss': 0.3936, 'grad_norm': 0.6691496968269348, 'learning_rate': 1.1555555555555556e-06, 'epoch': 2.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b73d0e568ba47efaea645328f6d591c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3964140713214874, 'eval_runtime': 11.8643, 'eval_samples_per_second': 84.287, 'eval_steps_per_second': 10.536, 'epoch': 3.0}\n",
      "{'train_runtime': 953.5335, 'train_samples_per_second': 28.316, 'train_steps_per_second': 3.539, 'train_loss': 0.5228604888916015, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../models/t5_summarizer_balanced\\\\tokenizer_config.json',\n",
       " '../models/t5_summarizer_balanced\\\\special_tokens_map.json',\n",
       " '../models/t5_summarizer_balanced\\\\spiece.model',\n",
       " '../models/t5_summarizer_balanced\\\\added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=models_path + \"t5_summarizer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=models_path + \"logs\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(models_path + \"t5_summarizer_balanced\")\n",
    "tokenizer.save_pretrained(models_path + \"t5_summarizer_balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f61df8",
   "metadata": {},
   "source": [
    "# Testing finetune T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c7a87",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c442b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mohamed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mohamed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Mohamed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8316f17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initial model Loading\n",
    "initial_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "initial_model.to(device)\n",
    "\n",
    "# Finetuned model Loading\n",
    "finetuned_model_path = \"../models/t5_summarizer_balanced\"\n",
    "finetuned_model = T5ForConditionalGeneration.from_pretrained(finetuned_model_path)\n",
    "finetuned_tokenizer = T5Tokenizer.from_pretrained(finetuned_model_path)\n",
    "finetuned_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a65782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model_to_eval, dataset, tokenizer, max_input_length=512, max_target_length=64\n",
    "):\n",
    "    model_to_eval.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for example in dataset.select(range(200)):\n",
    "        input_text = \"summarize: \" + example[\"input_text\"]\n",
    "        input_ids = tokenizer.encode(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_input_length,\n",
    "        )\n",
    "        input_ids = input_ids.to(model_to_eval.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model_to_eval.generate(input_ids, max_length=max_target_length)\n",
    "        pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        predictions.append(pred)\n",
    "        references.append(example[\"target_text\"])\n",
    "\n",
    "    results = {}\n",
    "    # ROUGE-L\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=predictions, references=references, use_stemmer=True\n",
    "    )\n",
    "    results[\"rougeL\"] = rouge_results[\"rougeL\"]\n",
    "    # BLEU score\n",
    "    bleu_results = bleu.compute(\n",
    "        predictions=predictions, references=[[ref] for ref in references]\n",
    "    )\n",
    "    results[\"bleu\"] = bleu_results[\"bleu\"]\n",
    "    # METEOR score\n",
    "    meteor_results = meteor.compute(predictions=predictions, references=references)\n",
    "    results[\"meteor\"] = meteor_results[\"meteor\"]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5069c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Metrics:\n",
      "ROUGE-L (Initial pre-trained model): 0.1027\n",
      "BLEU (Initial pre-trained model): 0.0059\n",
      "METEOR (Initial pre-trained model): 0.1401\n",
      "\n",
      "Fine-tuned Model Metrics:\n",
      "ROUGE-L (Fine-tuned model): 0.1614\n",
      "BLEU (Fine-tuned model): 0.0289\n",
      "METEOR (Fine-tuned model): 0.0999\n"
     ]
    }
   ],
   "source": [
    "# Initial model\n",
    "initial_results = evaluate_model(initial_model, eval_dataset, tokenizer)\n",
    "print(\"Initial Model Metrics:\")\n",
    "print(f\"ROUGE-L (Initial pre-trained model): {initial_results['rougeL']:.4f}\")\n",
    "print(f\"BLEU (Initial pre-trained model): {initial_results['bleu']:.4f}\")\n",
    "print(f\"METEOR (Initial pre-trained model): {initial_results['meteor']:.4f}\")\n",
    "\n",
    "# Fine-tuned model\n",
    "finetuned_results = evaluate_model(finetuned_model, eval_dataset, tokenizer)\n",
    "print(\"\\nFine-tuned Model Metrics:\")\n",
    "print(f\"ROUGE-L (Fine-tuned model): {finetuned_results['rougeL']:.4f}\")\n",
    "print(f\"BLEU (Fine-tuned model): {finetuned_results['bleu']:.4f}\")\n",
    "print(f\"METEOR (Fine-tuned model): {finetuned_results['meteor']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_outputs(\n",
    "    initial_model,\n",
    "    finetuned_model,\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    sample_size=5,\n",
    "    max_input_length=512,\n",
    "    max_target_length=64,\n",
    "    target_text=True,\n",
    "):\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    random_indices = random.sample(indices, sample_size)\n",
    "\n",
    "    for idx in random_indices:\n",
    "        example = dataset[idx]\n",
    "        input_str = \"summarize: \" + example[\"input_text\"]\n",
    "        input_ids = tokenizer.encode(\n",
    "            input_str, return_tensors=\"pt\", truncation=True, max_length=max_input_length\n",
    "        ).to(device)\n",
    "\n",
    "        # Initial model\n",
    "        with torch.no_grad():\n",
    "            initial_output_ids = initial_model.generate(\n",
    "                input_ids, max_length=max_target_length\n",
    "            )\n",
    "        initial_output = tokenizer.decode(\n",
    "            initial_output_ids[0], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Fine-tuned model\n",
    "        with torch.no_grad():\n",
    "            finetuned_output_ids = finetuned_model.generate(\n",
    "                input_ids, max_length=max_target_length\n",
    "            )\n",
    "        finetuned_output = tokenizer.decode(\n",
    "            finetuned_output_ids[0], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Display the outputs\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Example ID: {idx}\")\n",
    "        print(\"Input Text:\")\n",
    "        print(example[\"input_text\"])\n",
    "        if target_text:\n",
    "            print(\"\\nReference Summary:\")\n",
    "            print(example[\"target_text\"])\n",
    "        print(\"\\nInitial Model Output:\")\n",
    "        print(initial_output)\n",
    "        print(\"\\nFine-tuned Model Output:\")\n",
    "        print(finetuned_output)\n",
    "        print(\"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e90857",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sample_outputs(initial_model, model, eval_dataset, tokenizer, device, sample_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d142bd",
   "metadata": {},
   "source": [
    "Based on some of the generated summaries, We can notice that there might be data imbalance as most of the fine-tuned outputs are positive even if the review is negative. Further investigation required. Also, some of the reviews from the dataset does not have a good summarization. Some examples are mentioned below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83f5f3",
   "metadata": {},
   "source": [
    "==================================================\n",
    "\n",
    "Example ID: 25 (Irrelevant summary)\n",
    "\n",
    "Input Text:\n",
    "Good flavor, but not real sweet.  I add a little stevia for my sweet tooth.\n",
    "\n",
    "Reference Summary:\n",
    "Mom of 5\n",
    "\n",
    "Initial Model Output:\n",
    "good flavor, but not real sweet. add a little stevia for my sweet tooth.\n",
    "\n",
    "Fine-tuned Model Output:\n",
    "Good flavor, but not real sweet\n",
    "\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0ba141",
   "metadata": {},
   "source": [
    "==================================================\n",
    "\n",
    "Example ID: 759 (Imbalanced data)\n",
    "\n",
    "Input Text:\n",
    "This Item Taste Like Dirt.. I've Prob Used it 4 Times & Now It's Just Sitting in MY Freezer.. I Have A High Tolerance for Nasty Stuff.. Just Don't Really Like this Product.. Something In Grinding It Up Makes It Taste Nasty.. The Hulled Seeds Nutiva Sells Are Way Better.. If You Want Good Tasting Hemp Protein Powder It's $15/lb @ Earthshiftproducts.com  but It Taste Wayyy Better Actually Taste Good From Earthshift..\n",
    "\n",
    "Reference Summary:\n",
    "Taste Really Gross\n",
    "\n",
    "Initial Model Output:\n",
    "I've Prob Used it 4 times & Now It's Just Sitting in MY Freezer.. I have a high tolerance for Nasty Stuff..\n",
    "\n",
    "Fine-tuned Model Output:\n",
    "Good Taste\n",
    "\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a2a056",
   "metadata": {},
   "source": [
    "# Testing on test dataset for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4f16e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Example ID: 8\n",
      "Input Text:\n",
      "I've been looking for a tasty cola that does not contain aspertame. This drink does not meet these standards. Tasty it is not. I tried to drink it and served it to four family members. They all disliked it. Zevia gets two stars though because it is an effective cleaner when it is mixed with baking soda.\n",
      "\n",
      "Initial Model Output:\n",
      "cola does not contain aspertame. it is an effective cleaner when mixed with baking soda.\n",
      "\n",
      "Fine-tuned Model Output:\n",
      "Not aspertame\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Example ID: 7\n",
      "Input Text:\n",
      "This cream soda is delicious but beware of packaging because one can was almost empty and the low box and plastic wrapping it was banged up. I suggest just buying it at a brick and morter store like walmart.\n",
      "\n",
      "Initial Model Output:\n",
      "cream soda is delicious but beware of packaging because one can was almost empty.\n",
      "\n",
      "Fine-tuned Model Output:\n",
      "Cream soda\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Example ID: 2\n",
      "Input Text:\n",
      "Love this drink with a little squeeze of lemon. Other people I have shared this with enjoy the squeeze of lemon as well. I just buy lemon in the bottle to make it easy. I takes away any of the stevia aftertaste that can exist. Love the fact that Zevia comes without the caffeine as well. Gives me a lot of comfort while trying to be more healthy.\n",
      "\n",
      "Initial Model Output:\n",
      "stevia is a caffeine-free drink.\n",
      "\n",
      "Fine-tuned Model Output:\n",
      "Love this drink\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Example ID: 5\n",
      "Input Text:\n",
      "There is nothing \"cola\" about the cola flavor. It tastes nothing like Pepsi, Coke, Diet versions, store brand generics, etc. This is one of the worst sparkling beverages I have ever drank. I think I would rather drink a 12 oz bottle of lemon-lime magnesium citrate (colonoscopy prep anyone?).\n",
      "\n",
      "Initial Model Output:\n",
      "cola is one of the worst sparkling beverages I have ever drank. it tastes nothing like Pepsi, Coke, Diet versions, store brand generics.\n",
      "\n",
      "Fine-tuned Model Output:\n",
      "Not cola\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Example ID: 1\n",
      "Input Text:\n",
      "I absolutely love Zevia Ginger Ale! I buy them every single week (or every other week) because I seriously can’t get enough. Finally found this pack on Amazon for the best price I’ve seen anywhere—total game-changer. The flavor is super refreshing, and it totally hits the spot when I’m craving soda, but without all the bad stuff regular soda has. No sugar, no weird ingredients—just clean, fizzy goodness. I’m completely obsessed and will definitely be reordering. Highly recommend if you’re trying to cut out traditional soda but still want something tasty\n",
      "\n",
      "Initial Model Output:\n",
      "I absolutely love Zevia Ginger Ale! I buy them every single week (or every other week) because I seriously can’t get enough. finally found this pack on amazon for the best price I’ve seen anywhere.\n",
      "\n",
      "Fine-tuned Model Output:\n",
      "Love this product!\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Example ID: 6\n",
      "Input Text:\n",
      "The soda is wonderful and I now order 4 cases a month. HOWEVER. THE PACKING IS HORRIBLE AND THE LAST 2 CASES LOOKED LINE THEY WERE TOSSED IN A BOX AND THE CARDBOARD CASES WERE TOSSED ON TOP. 6 OF THR CANS HAD SUCH A BAD DENT IN THE BOTTOM THEY HAVE TO BE STORED UPSIDE DOWN. Fix the shipping and this is a 5 star product.\n",
      "\n",
      "Initial Model Output:\n",
      "6 of THR CANS HAD SUCH a BAD DENT IN THE BOTTOM THEY HAVE TO BE STORED UPSIDE DOWN.\n",
      "\n",
      "Fine-tuned Model Output:\n",
      "HORRIBLE\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Example ID: 3\n",
      "Input Text:\n",
      "I have written a lot of reviews and can't remember leaving a bad one.........until now. These are absolutely the grossest tasting drinks I have ever had the misfortune of allowing beyond my lips. I got the variety pack and have tasted each one thinking if only ONE was a hit, it might be worth the price.  Let me make it clear that NONE of them are even remotely flavorful. All of them have a chemical taste that is immediately detectable. If that is not bad enough, they also leave a bitter aftertaste that lingers for a LONG time. I honestly compare them to some sort of medicine that I would tell my doctor I'm not taking again! Now I have all of these cans of this crap that I don't know what to do with. I can't give them to friends because I wouldn't have them anymore if I did. Trust me guys, look elsewhere. These are unbelievably bad!\n",
      "\n",
      "Initial Model Output:\n",
      "these are absolutely the grossest tasting drinks I have ever had. I have tasted each one thinking if only ONE was a hit, it might be worth the price. they leave a bitter aftertaste that lingers for a LONG time.\n",
      "\n",
      "Fine-tuned Model Output:\n",
      "YUM!\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Example ID: 10\n",
      "Input Text:\n",
      "Not Cola. Tasty and no sugar but no way does it taste like cola. I won’t order again. But I’ll enjoy it even though I’m a big fan of Rum with Cherry Coke.\n",
      "\n",
      "Initial Model Output:\n",
      "cola is a big fan of Rum with Cherry Coke.\n",
      "\n",
      "Fine-tuned Model Output:\n",
      "Tasty and no sugar\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Example ID: 9\n",
      "Input Text:\n",
      "While these are drinkable, they suffer from a pronounced core flavor that's basically the same for all the ones I've tried so far, and all with a taste far weaker than most soda. For those looking for a stevia-based cola that actually tastes like one, try Green Cola.\n",
      "\n",
      "Initial Model Output:\n",
      "green cola is a stevia-based cola that tastes like one. it's a stevia-based cola that's much weaker than most sodas.\n",
      "\n",
      "Fine-tuned Model Output:\n",
      "Green Cola\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Example ID: 11\n",
      "Input Text:\n",
      "Standard size. Nice and carbonated and fizzy. Smells like stevia and soda. Tastes good. I bought these because I was drinking too much diet coke, and diet coke makes me more hungry. I didn't have that issue with these Zevias. Only thing is that Stevia can leave a bit of a bitter aftertaste, but I don't mind it too much. It helps to drink these super cold, straight out of the fridge.\n",
      "\n",
      "Initial Model Output:\n",
      "stevia and soda are good. I bought these because I was drinking too much diet coke.\n",
      "\n",
      "Fine-tuned Model Output:\n",
      "Great for a good price\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Example ID: 12\n",
      "Input Text:\n",
      "This Item Taste Like Dirt.. I've Prob Used it 4 Times & Now It's Just Sitting in MY Freezer.. I Have A High Tolerance for Nasty Stuff.. Just Don't Really Like this Product.. Something In Grinding It Up Makes It Taste Nasty.. The Hulled Seeds Nutiva Sells Are Way Better.. If You Want Good Tasting Hemp Protein Powder It's $15/lb @ Earthshiftproducts.com  but It Taste Wayyy Better Actually Taste Good From Earthshift..\n",
      "\n",
      "Initial Model Output:\n",
      "I've Prob Used it 4 times & Now It's Just Sitting in MY Freezer.. I have a high tolerance for Nasty Stuff..\n",
      "\n",
      "Fine-tuned Model Output:\n",
      "Not Worth It\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Example ID: 4\n",
      "Input Text:\n",
      "Too bad it wasn't out of six stars so I could give it one star out of six. The black cherry was the only flavor that I actually enjoyed. The vanilla cola came close which is funny because I don't really like cola. The other four are nasty. The only other redeeming quality is that it was a variety pack so I could learn with just one purchase that I don't really like this brand of soda. If they sell black cherry separately I might actually purchase a batch but never any other flavor from them.\n",
      "\n",
      "Initial Model Output:\n",
      "black cherry was the only flavor that I actually enjoyed. the vanilla cola came close.\n",
      "\n",
      "Fine-tuned Model Output:\n",
      "Not a good flavor\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Example ID: 0\n",
      "Input Text:\n",
      "All flavors are great except the regular cola one. It’s not bad but lacking taste. Love the cream soda and root beer.\n",
      "\n",
      "Initial Model Output:\n",
      "all flavors are great except the regular cola one.\n",
      "\n",
      "Fine-tuned Model Output:\n",
      "Not bad\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(data_path + \"test_final_presentation.csv\", usecols=[\"Review\"])\n",
    "test_df.rename(columns={\"Review\": \"input_text\"}, inplace=True)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df)  # Convert to Hugging Face Dataset format\n",
    "\n",
    "sample_outputs(\n",
    "    initial_model,\n",
    "    finetuned_model,\n",
    "    test_dataset,\n",
    "    finetuned_tokenizer,\n",
    "    device,\n",
    "    sample_size=13,\n",
    "    target_text=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
