{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "from evaluate import load\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    BartTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import kagglehub\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version of amazon fine food reviews dataset\n",
    "path = kagglehub.dataset_download(\"snap/amazon-fine-food-reviews\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the csv for the summary and text columns and use them for the input and target\n",
    "df = pd.read_csv(\"Reviews.csv\", usecols=[\"Id\", \"Summary\", \"Text\", \"ProductId\"])\n",
    "df.dropna(subset=[\"Summary\", \"Text\"], inplace=True)\n",
    "df = df.sample(20000, random_state=42)\n",
    "df = df.rename(columns={\"Summary\": \"target_text\", \"Text\": \"input_text\"})\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize the pretrained tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained(\"lucadiliello/bart-small\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"lucadiliello/bart-small\")\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproccessing function for tokenizing and padding the dataset\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the input text with padding and truncation to a max length of 512\n",
    "\n",
    "    return tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    ) | {\n",
    "        \"labels\": tokenizer(  # Tokenize the target text separately, with a shorter max length for summaries\n",
    "            examples[\"target_text\"],\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )[\n",
    "            \"input_ids\"\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize dataset and slit into train and eval\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging purpose using a smaller set\n",
    "small_train = train_dataset  # .select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using these hyperparams and save to ./bart_summarizer folder\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart_summarizer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,  # number of epochs is small enough to train quicker but also to learn enough\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# save model to ./bart_summarizer folder\n",
    "model.save_pretrained(\"./bart_summarizer\")\n",
    "tokenizer.save_pretrained(\"./bart_summarizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rouge score evaluator\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "# call the initial model and the finetuned model so we can compare them\n",
    "initial_model = BartForConditionalGeneration.from_pretrained(\"lucadiliello/bart-small\")\n",
    "\n",
    "finetuned_model = BartForConditionalGeneration.from_pretrained(\"./bart_summarizer\")\n",
    "finetuned_tokenizer = BartTokenizer.from_pretrained(\"./bart_summarizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Metrics:\n",
      "ROUGE-L (Initial pre-trained model): 0.0711\n",
      "\n",
      "Fine-tuned Model Metrics:\n",
      "ROUGE-L (Fine-tuned model): 0.1335\n"
     ]
    }
   ],
   "source": [
    "# evaluation function to get the rouge score\n",
    "def evaluate_model(\n",
    "    model_to_eval, dataset, tokenizer, max_input_length=512, max_target_length=64\n",
    "):\n",
    "    model_to_eval.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # evaluate on the first 200 examples\n",
    "    for example in dataset.select(range(200)):\n",
    "        input_text = example[\"input_text\"]\n",
    "        input_ids = tokenizer.encode(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_input_length,\n",
    "        )\n",
    "        input_ids = input_ids.to(model_to_eval.device)\n",
    "\n",
    "        # generate prediction from the model\n",
    "        with torch.no_grad():\n",
    "            output_ids = model_to_eval.generate(input_ids, max_length=max_target_length)\n",
    "        pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        predictions.append(pred)\n",
    "        references.append(example[\"target_text\"])\n",
    "\n",
    "    results = {}\n",
    "    # ROUGE-L\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=predictions, references=references, use_stemmer=True\n",
    "    )\n",
    "    results[\"rougeL\"] = rouge_results[\"rougeL\"]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Initial model\n",
    "initial_results = evaluate_model(initial_model, eval_dataset, tokenizer)\n",
    "print(\"Initial Model Metrics:\")\n",
    "print(f\"ROUGE-L (Initial pre-trained model): {initial_results['rougeL']:.4f}\")\n",
    "\n",
    "# Fine-tuned model\n",
    "finetuned_results = evaluate_model(finetuned_model, eval_dataset, tokenizer)\n",
    "print(\"\\nFine-tuned Model Metrics:\")\n",
    "print(f\"ROUGE-L (Fine-tuned model): {finetuned_results['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: Great cinnamon bears\n"
     ]
    }
   ],
   "source": [
    "# simple test to see if a coherent summary is generated\n",
    "input_text = \"These cinnamon bears have great flavor and do not taste sugar free.  My only issue is that they should be softer.\"\n",
    "inputs = finetuned_tokenizer.encode(\n",
    "    input_text, return_tensors=\"pt\", max_length=512, truncation=True\n",
    ")\n",
    "summary_ids = finetuned_model.generate(\n",
    "    inputs, max_length=64, num_beams=4, early_stopping=True\n",
    ")\n",
    "\n",
    "summary = finetuned_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Id', 'ProductId', 'target_text', 'input_text', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "\n",
      "Input: I am a vegetarian body builder. I have tried many types of plant protein supplimentation. Hemp is the gold standard of the plant kingdom, much like eggs are the gold standard of the animal kingdom. Their amino acid profiles are both very complete and similar.<br /><br />Most hemp proteins have a massive amount of fiber though and if you are taking in as many grams of protein as recommended for body building, you will be way too regular I'll put it that way.<br /><br />This 70% formula has less fiber and more protein. In one lb/16oz container there are 15 servings of 21g of protein 0g fiber. It is superior for that alone and is well worth the extra cost per/gram of protein.<br /><br />It disolves easily in water, you can hand mix with a spoon for 1min and your good with not one clump.<br /><br />It is sweeter than normal hemp. Hard to explain, it is a very mild taste compared to other hemp proteins. It's like the bitterness and overpowering herb flavor gets filtered out when the fiber gets removed.<br /><br />I'm glad someone made hemp a pleasure to drink instead of a chore!<br /><br />Hope this review helped.\n",
      "Generated Summary: Good Stuff!\n",
      "\n",
      "Input: Coffee is very weak,I have to use 3 packs to make 4 cups of coffee.\n",
      "Generated Summary: Coffee weak\n",
      "\n",
      "Input: I had gastric sleeve surgery feb2012 and did not care for the protein shakes , so i turned to sugar free instant breakfast i drink 2x a day! I only wish they sold just strawberry, i dislike the vanilla and sometimes get tired of the chocolate. Have written the company with no reply just a coupon!!\n",
      "Generated Summary: Yummy!\n",
      "\n",
      "Input: Two Leaves and a Bud Earl Grey is my absolute favorite morning tea. Rich flavor, and perfect with a dab of raw honey.\n",
      "Generated Summary: My favorite morning tea\n",
      "\n",
      "Input: Bob's Red Mill 13 Bean Soup Mix allows you to make an easy, great-tasting, very filling bean soup that will last for days. Love it!\n",
      "Generated Summary: Great product!\n"
     ]
    }
   ],
   "source": [
    "# Sample evaluations from the eval dataset and the finetuned model\n",
    "print(eval_dataset)\n",
    "indices = list(range(len(eval_dataset)))\n",
    "random_indices = random.sample(indices, 5)\n",
    "\n",
    "for idx in random_indices:\n",
    "    # get the example from the eval dataset\n",
    "    example = eval_dataset[idx]\n",
    "\n",
    "    input_str = example[\"input_text\"]\n",
    "    print(\"\\nInput:\", input_str)\n",
    "    # use finetuned model to generate a summary, tokenize the input and generate the summary\n",
    "    inputs = finetuned_tokenizer(\n",
    "        input_str, return_tensors=\"pt\", max_length=512, truncation=True\n",
    "    )\n",
    "\n",
    "    summary_ids = finetuned_model.generate(\n",
    "        **inputs, max_length=64, num_beams=4, early_stopping=True\n",
    "    )\n",
    "    summary = finetuned_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Generated Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
